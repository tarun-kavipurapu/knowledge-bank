**Zero-Shot Learning**: If the model is able to learn without any examples

**One-Shot Learning:** if the model is able to learn with only one example 

**Few-Shot Learning:** If the model is able to learn with only few examples

All GPT's till now are Few-shot but they also have zero-shot 



### Little bit on How the Next word predictions happens?
**An autoregressive model predicts future values based on past values.** The way the autoregressive generative NLP model actually work is that after each token is produced, that token is added to the sequence of inputs. And that new sequence becomes the input to the model in its next step. For example, the user initializes the input as “recite the first law $”, where “$” is a special delimiter token. GPT model will generate the text autoregressively, conditioned on the user input.
![[Pasted image 20250115150751.png]]

Although GPT is trained for Next word prediction it performes many other tasks like chat bot and language translation this is called **emergent behaviour**



Original Transformer had 6 encoder-decoder blocks where as GPT-3 had 96 transformer layers,175 billion parameters



# Stages of Building LLM's

![[Pasted image 20250115152456.png]]