Secret Sause is Transformers

### LLM has two Major Steps
1. Pretraining: Training on Large general Dataset 
	1. this is done mostly on unlabelled data which is also called autoregressive data
2. FInetuning:Finetuning to a particular narrower dataset
	1. fine tuning is mostly done to a labelled data
	2. fine training can be further classified to instruction fine-tuning and  fine- tuning on classification tasks 
### Transformer (attention is all you need )
- This was originally developed for machine language translation(english to french )
- LLMS are not same as Transformers transformer is an architecture based on which many llms have been made
- llms can be made with many architectures like RNN and LSTMS
### Simplified Transformer Architecture for Understanding
![[Pasted image 20250111180601.png]]
- This is an Simplified architecture complete architecture will be discussed further
- Learn More about How Vector Embedding is done to absorb the context 
- GPT(Generative Pretrained Transformer) 
- BERT(**Bidirectional Encoder Representations from Transformers**) 
- There is a DIfference Between Them 
- ![[Pasted image 20250111181141.png]]
- Observe that GPT and BERT have different architecture than Transformer 
- GPT has only decoder 
- Since BERT captures the context bidirectionally it is much better at Sentiment analysis